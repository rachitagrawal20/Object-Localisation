{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Localisation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuickLearner171998/Object-Loc/blob/master/Object_Localisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cUEGKR4nGgxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pxs0erAZGsn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Authorise colab\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dKd1msiwHBvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# to get files directly from drive\n",
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ot3gOXIPM9ct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # To use this code as it is make the text files in the same format.\n",
        "\n",
        "1) Make a train_labels.txt file which contains image name and image id .\n",
        "     Format: id img_name\n",
        "    \n",
        "2) Make a train_coords.txt with format: id x1 y1 w h .\n",
        "     x1,y1 are the top left corner, and  w, h are the width and height of the bounding box.\n",
        "     \n",
        "3) Create testid.txt which contains id and  test image names\n",
        "     format: id img_name"
      ]
    },
    {
      "metadata": {
        "id": "r9nISMsziZhj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing training dataset\n",
        "\n",
        "You can use **LabelImg**. It is an open source software for drawing bounding boxes around the objects in images. From this you can easily get the coordinates of bounding boxes for train_coords."
      ]
    },
    {
      "metadata": {
        "id": "CuAARdZKlHsS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Get_data_train"
      ]
    },
    {
      "metadata": {
        "id": "LISba-xjX9Jq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def smooth_l1_loss(true_box,pred_box):\n",
        "    loss=0.0\n",
        "    for i in range(4):\n",
        "        residual=tf.abs(true_box[:,i]-pred_box[:,i]*224)\n",
        "        condition=tf.less(residual,1.0)\n",
        "        small_res=0.5*tf.square(residual)\n",
        "        large_res=residual-0.5\n",
        "        loss=loss+tf.where(condition,small_res,large_res)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def Normalize(image,mean,std):\n",
        "    for channel in range(3):\n",
        "        image[:,:,channel]=(image[:,:,channel]-mean[channel])/std[channel]\n",
        "    return image\n",
        "  \n",
        "  \n",
        "# creating a custom data generator.  \n",
        "def Imagegen(start,batchsize):\n",
        "    id_to_data={}\n",
        "    #id_to_size={}   \n",
        "    #use id_to_size when the size of all images is not same.\n",
        "    j=1\n",
        "    i=start\n",
        "    \n",
        "    with open(\"train_labels.txt\") as f:\n",
        "        lines=f.read().splitlines()\n",
        "        for line in lines:\n",
        "            if (j>=i) and (j<i+batchsize):\n",
        "                #id is the serial number of tha image starting  from 1\n",
        "                id,path=line.split(\" \",1)\n",
        "                image=Image.open(\"train_images/\"+path).convert('RGB')\n",
        "                #id_to_size[int(id)]=np.array(image,dtype=np.float32).shape[0:2]                \n",
        "                image=image.resize((224,224))\n",
        "                image=np.array(image,dtype=np.float32)\n",
        "                image=image/255\n",
        "                image=Normalize(image,[0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "                id_to_data[int(id)]=image                \n",
        "                \n",
        "            j=j+1\n",
        "    id_to_data=np.array(list(id_to_data.values()))\n",
        "    #id_to_size=np.array(list(id_to_size.values()))\n",
        "    f=open(\"id_to_data\",\"wb+\")\n",
        "    pickle.dump(id_to_data,f,protocol=4)\n",
        "   # f=open(\"id_to_size\",\"wb+\")\n",
        "   # pickle.dump(id_to_size,f,protocol=4)\n",
        "    \n",
        "    \n",
        "   \n",
        "    j=1\n",
        "    k=1\n",
        "    i=start\n",
        "    width = 640\n",
        "    height = 480\n",
        "\n",
        "    id_to_box={}\n",
        "    with open(\"train_coords.txt\") as f:\n",
        "        lines=f.read().splitlines()\n",
        "        for line in lines:\n",
        "            if (j>=i) and (j<i+batchsize):\n",
        "                id,box=line.split(\" \",1)\n",
        "                box=np.array([float(i) for i in box.split(\" \")],dtype=np.float32)\n",
        "                box[0]=box[0]/width*224\n",
        "                box[1]=box[1]/height*224\n",
        "                box[2]=box[2]/width*224\n",
        "                box[3]=box[3]/height*224\n",
        "                \n",
        "#                 if size of images different                \n",
        "#                 box[0]=box[0]/id_to_size[int(id)-1][1]*224\n",
        "#                 box[1]=box[1]/id_to_size[int(id)-1][0]*224\n",
        "#                 box[2]=box[2]/id_to_size[int(id)-1][1]*224\n",
        "#                 box[3]=box[3]/id_to_size[int(id)-1][0]*224\n",
        "\n",
        "                id_to_box[int(k)]=box\n",
        "                k=k+1\n",
        "            j=j+1\n",
        "    id_to_box=np.array(list(id_to_box.values()))\n",
        "    f=open(\"id_to_box\",\"wb+\") \n",
        "    pickle.dump(id_to_box,f,protocol=4)\n",
        "    return id_to_box,id_to_data\n",
        "    \n",
        "i=1\n",
        "batchsize=1000\n",
        "#number of total images = 24000\n",
        "for i in range(1,24000,batchsize):\n",
        "    id_to_box,id_to_data = Imagegen(i,batchsize)\n",
        "    f=open(\"id_to_box\",\"wb+\") \n",
        "    pickle.dump(id_to_box,f,protocol=4)\n",
        "    f=open(\"id_to_data\",\"wb+\")\n",
        "    pickle.dump(id_to_data,f,protocol=4)\n",
        "    with open(\"final_id_to_data\", \"ab\") as myfile, open(\"id_to_data\", \"rb\") as file2:\n",
        "      myfile.write(file2.read())\n",
        "      with open(\"final_id_to_box\", \"ab\") as myfile, open(\"id_to_box\", \"rb\") as file2:\n",
        "      myfile.write(file2.read())\n",
        "    f =open(\"final_id_to_box\",\"ab\")\n",
        "    pickle.dump(id_to_box,f,protocol=4)\n",
        "    f = open(\"final_id_to_data\",\"ab\")\n",
        "    pickle.dump(id_to_data,f,protocol=4)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yF4Viip7lPIi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train"
      ]
    },
    {
      "metadata": {
        "id": "yvzXUvHY2n_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model,load_model\n",
        "from keras.callbacks import ModelCheckpoint,LearningRateScheduler,ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import keras.losses\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "# data: Train-23000, test-1000\n",
        "\n",
        "def getdata():\n",
        "  train_data = 23000\n",
        "  test_data=1000\n",
        "  btch_size =1000\n",
        "  runs = train_data/btch_size\n",
        "  tst_run = test_data/btch_size\n",
        " # read data and shuffle\n",
        "  index=[i for i in range(test_size)]\n",
        "  random.shuffle(index)\n",
        "\n",
        "  \n",
        "  f=open(\"final_id_to_data\",\"rb\")\n",
        "  data = pickle.load(f)\n",
        "  for j in range(0,runs):\n",
        "    if j==(runs - tst_runs):\n",
        "      data1 = pickle.load(f)\n",
        "    elif j>(runs - tst_runs):\n",
        "      data1 = np.concatenate((data1, pickle.load(f)), axis=0)\n",
        "    else: \n",
        "      data = pickle.load(f)\n",
        "  data_test=data1[index]\n",
        "  \n",
        "  \n",
        "  f=open(\"final_id_to_box\",\"rb\")\n",
        "  box=pickle.load(f)\n",
        "  for j in range(0,runs):\n",
        "    if j==(runs - tst_runs):\n",
        "      box1 = pickle.load(f)\n",
        "    elif j>(runs - tst_runs):\n",
        "      box1 = np.concatenate((box1, pickle.load(f)), axis=0)\n",
        "    else: \n",
        "      box = pickle.load(f)\n",
        "  box_test=box1[index]\n",
        "  return data_test,box_test\n",
        "\n",
        "def plot_model(model_details):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "\n",
        "    axs[0].plot(range(1,len(model_details.history['my_metric'])+1),model_details.history['my_metric'])\n",
        "    axs[0].plot(range(1,len(model_details.history['val_my_metric'])+1),[1.7*x for x in model_details.history['val_my_metric']])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_details.history['my_metric'])+1),len(model_details.history['my_metric'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[1].plot(range(1,len(model_details.history['loss'])+1),model_details.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_details.history['val_loss'])+1),model_details.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_details.history['loss'])+1),len(model_details.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    plt.savefig(\"model.png\")\n",
        "\n",
        "\n",
        "    \n",
        "# metric function\n",
        "def my_metric(labels,predictions):\n",
        "    threshhold=0.75\n",
        "    x=predictions[:,0]*224\n",
        "    x=tf.maximum(tf.minimum(x,224.0),0.0)\n",
        "    y=predictions[:,1]*224\n",
        "    y=tf.maximum(tf.minimum(y,224.0),0.0)\n",
        "    width=predictions[:,2]*224\n",
        "    width=tf.maximum(tf.minimum(width,224.0),0.0)\n",
        "    height=predictions[:,3]*224                   # we need width + x\n",
        "    height=tf.maximum(tf.minimum(height,224.0),0.0) # we need height +y\n",
        "    label_x=labels[:,0]\n",
        "    label_y=labels[:,1]\n",
        "    label_width=labels[:,2]\n",
        "    label_height=labels[:,3]\n",
        "    a1=tf.multiply(width,height)\n",
        "    a2=tf.multiply(label_width,label_height)\n",
        "    x1=tf.maximum(x,label_x)\n",
        "    y1=tf.maximum(y,label_y)\n",
        "    x2=tf.minimum(x+width,label_x+label_width)\n",
        "    y2=tf.minimum(y+height,label_y+label_height)\n",
        "    IoU=tf.abs(tf.multiply((x1-x2),(y1-y2)))/(a1+a2-tf.abs(tf.multiply((x1-x2),(y1-y2))))\n",
        "    condition=tf.less(threshhold,IoU)\n",
        "    sum=tf.where(condition,tf.ones(tf.shape(condition)),tf.zeros(tf.shape(condition)))\n",
        "    return tf.reduce_mean(sum)\n",
        "\n",
        "# loss function\n",
        "def smooth_l1_loss(true_box,pred_box):\n",
        "    loss=0.0\n",
        "    for i in range(4):\n",
        "        residual=tf.abs(true_box[:,i]-pred_box[:,i]*224)\n",
        "        condition=tf.less(residual,1.0)\n",
        "        small_res=0.5*tf.square(residual)\n",
        "        large_res=residual-0.5\n",
        "        loss=loss+tf.where(condition,small_res,large_res)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "def resnet_block(inputs,num_filters,kernel_size,strides,activation='relu'):\n",
        "    x=Conv2D(num_filters,kernel_size=kernel_size,strides=strides,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(inputs)\n",
        "    x=BatchNormalization()(x)\n",
        "    if(activation):\n",
        "        x=Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet18():\n",
        "    inputs=Input((224,224,3))\n",
        "    \n",
        "    # conv1\n",
        "    x=resnet_block(inputs,64,[7,7],2)\n",
        "\n",
        "    # conv2\n",
        "    x=MaxPooling2D([3,3],2,'same')(x)\n",
        "    for i in range(2):\n",
        "        a=resnet_block(x,64,[3,3],1)\n",
        "        b=resnet_block(a,64,[3,3],1,activation=None)\n",
        "        x=keras.layers.add([x,b])\n",
        "        x=Activation('relu')(x)\n",
        "    \n",
        "    # conv3\n",
        "    a=resnet_block(x,128,[1,1],2)\n",
        "    b=resnet_block(a,128,[3,3],1,activation=None)\n",
        "    x=Conv2D(128,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    a=resnet_block(x,128,[3,3],1)\n",
        "    b=resnet_block(a,128,[3,3],1,activation=None)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    # conv4\n",
        "    a=resnet_block(x,256,[1,1],2)\n",
        "    b=resnet_block(a,256,[3,3],1,activation=None)\n",
        "    x=Conv2D(256,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    a=resnet_block(x,256,[3,3],1)\n",
        "    b=resnet_block(a,256,[3,3],1,activation=None)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    # conv5\n",
        "    a=resnet_block(x,512,[1,1],2)\n",
        "    b=resnet_block(a,512,[3,3],1,activation=None)\n",
        "    x=Conv2D(512,kernel_size=[1,1],strides=2,padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(x)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    a=resnet_block(x,512,[3,3],1)\n",
        "    b=resnet_block(a,512,[3,3],1,activation=None)\n",
        "    x=keras.layers.add([x,b])\n",
        "    x=Activation('relu')(x)\n",
        "\n",
        "    x=AveragePooling2D(pool_size=7,data_format=\"channels_last\")(x)\n",
        "    # out:1*1*512\n",
        "\n",
        "    y=Flatten()(x)\n",
        "    # out:512\n",
        "    y=Dense(1000,kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(y)\n",
        "    outputs=Dense(4,kernel_initializer='he_normal',kernel_regularizer=l2(1e-3))(y)\n",
        "    \n",
        "    model=Model(inputs=inputs,outputs=outputs)\n",
        "    return model\n",
        "  \n",
        "  \n",
        "keras.losses.smooth_l1_loss = smooth_l1_loss\n",
        "\n",
        "# If you are training the model for the first time\n",
        "model = resnet18()\n",
        "\n",
        "\n",
        "\n",
        "#if you are re-training a pretrained model\n",
        "# model = load_model('gdrive/My Drive/model.h5',custom_objects = {'smooth_l1_loss': smooth_l1_loss,'my_metric':my_metric})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=\"smooth_l1_loss\",optimizer=Adam(),metrics=[my_metric])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "def lr_sch(epoch):\n",
        "    #200 total\n",
        "    if epoch <50:\n",
        "        return 1e-3\n",
        "    if 50<=epoch<100:\n",
        "        return 1e-4\n",
        "    if epoch>=100:\n",
        "        return 1e-5\n",
        "\n",
        "# defining a custom data generator            \n",
        "def generator(batch_size):\n",
        "    \n",
        "    counter=0\n",
        "    epoch_counter = 0\n",
        "    data_size = 1000     # size of data loaded when you do pickle.load\n",
        "    \n",
        "    f=open(\"final_id_to_data\",\"rb\")\n",
        "    index=[i for i in range(data_size)]\n",
        "    random.shuffle(index)\n",
        "    X_data = pickle.load(f) \n",
        "    X_data = X_data[index]\n",
        "    f1=open(\"final_id_to_box\",\"rb\")\n",
        "    y_data = pickle.load(f1) \n",
        "    y_data = y_data[index]\n",
        "    samples_per_epoch = X_data.shape[0]\n",
        "    number_of_batches = samples_per_epoch/batch_size\n",
        "\n",
        "    while 1:\n",
        "\n",
        "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "        counter += 1\n",
        "        epoch_counter += 1\n",
        "        yield X_batch,y_batch\n",
        "\n",
        "        #restart counter to yeild data in the next epoch as well\n",
        "        if epoch_counter >= 200 :\n",
        "            epoch_counter = 0;\n",
        "            counter = 0;\n",
        "            f=open(\"final_id_to_data\",\"rb\")\n",
        "            index=[i for i in range(data_size)]\n",
        "            random.shuffle(index)\n",
        "            X_data = pickle.load(f) \n",
        "            X_data = X_data[index]\n",
        "            f1=open(\"final_id_to_box\",\"rb\")\n",
        "            y_data = pickle.load(f1) \n",
        "            y_data = y_data[index]\n",
        "            \n",
        "        if counter >= number_of_batches:\n",
        "            X_data = pickle.load(f)\n",
        "            counter = 0\n",
        "            X_data = X_data[index]\n",
        "            y_data = pickle.load(f1)\n",
        "            y_data = y_data[index]\n",
        "            \n",
        "            \n",
        "            \n",
        "data_test,box_test=getdata()\n",
        "            \n",
        "            \n",
        "def val_generator(X_data, y_data, batch_size):\n",
        "\n",
        "    samples_per_epoch = X_data.shape[0]\n",
        "    number_of_batches = samples_per_epoch/batch_size\n",
        "    counter=0\n",
        "\n",
        "    while 1:\n",
        "\n",
        "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "        counter += 1\n",
        "        yield X_batch,y_batch\n",
        "\n",
        "        #restart counter to yeild data in the next epoch as well\n",
        "        if counter >= number_of_batches:\n",
        "            counter = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "lr_scheduler=LearningRateScheduler(lr_sch)\n",
        "lr_reducer=ReduceLROnPlateau(monitor='val_my_metric',factor=0.2,patience=5,mode='max',min_lr=1e-3)\n",
        "\n",
        "checkpoint=ModelCheckpoint('model.h5',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
        "\n",
        "# model_details=model.fit(data_train,box_train,batch_size=128,epochs=100,shuffle=True,validation_split=0.1,callbacks=[lr_scheduler,lr_reducer,checkpoint],verbose=1)\n",
        "# model_details=model.fit_generator(generator(batch_size=100),steps_per_epoch = 200,epochs=100,verbose=1,callbacks=[lr_scheduler,lr_reducer,checkpoint])\n",
        "\n",
        "model_details=model.fit_generator(generator(batch_size=100),steps_per_epoch = 200,epochs=200,verbose=1,callbacks=[lr_scheduler,lr_reducer,checkpoint],validation_data=val_generator(data_test,box_test,100), validation_steps=(data_test.shape[0])/100)\n",
        "\n",
        "model.save('gdrive/My Drive/model.h5')\n",
        "\n",
        "scores=model.evaluate(data_test,box_test,verbose=1)\n",
        "print('Test loss : ',scores[0])\n",
        "print('Test accuracy : ',scores[1])\n",
        "\n",
        "plot_model(model_details)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTZTYYeGle9-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#GET_DATA_TEST"
      ]
    },
    {
      "metadata": {
        "id": "gg-7dRy5TcpH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "import random\n",
        "import math\n",
        "import keras.losses\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "#getdata\n",
        "def smooth_l1_loss(true_box,pred_box):\n",
        "    loss=0.0\n",
        "    for i in range(4):\n",
        "        residual=tf.abs(true_box[:,i]-pred_box[:,i]*224)\n",
        "        condition=tf.less(residual,1.0)\n",
        "        small_res=0.5*tf.square(residual)\n",
        "        large_res=residual-0.5\n",
        "        loss=loss+tf.where(condition,small_res,large_res)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def Normalize(image,mean,std):\n",
        "    for channel in range(3):\n",
        "        image[:,:,channel]=(image[:,:,channel]-mean[channel])/std[channel]\n",
        "    return image\n",
        "\n",
        "def Imagegen(start,batchsize):\n",
        "    id_to_data={}\n",
        "    id_to_size={}\n",
        "    j=1\n",
        "    i=start\n",
        "    \n",
        "    with open(\"{/content}/datasets/f2016759/testid/testid.txt\") as f:\n",
        "        lines=f.read().splitlines()\n",
        "        for line in lines:\n",
        "            if (j>=i) and (j<=i+batchsize):\n",
        "                id,path=line.split(\" \",1)\n",
        "                image=Image.open(\"{/content}/datasets/f2016759/test-images/test_images/\"+path).convert('RGB')\n",
        "                #print(np.array(image,dtype=np.float32).shape[0:2])\n",
        "                id_to_size[int(id)]=np.array(image,dtype=np.float32).shape[0:2]                \n",
        "                image=image.resize((224,224))\n",
        "                image=np.array(image,dtype=np.float32)\n",
        "                image=image/255\n",
        "                image=Normalize(image,[0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "                id_to_data[int(id)]=image                \n",
        "                \n",
        "            j=j+1\n",
        "    id_to_data=np.array(list(id_to_data.values()))\n",
        "    id_to_size=np.array(list(id_to_size.values()))\n",
        "    f=open(\"id_to_data\",\"wb+\")\n",
        "    pickle.dump(id_to_data,f,protocol=4)\n",
        "    f=open(\"id_to_size\",\"wb+\")\n",
        "    pickle.dump(id_to_size,f,protocol=4)\n",
        "    return id_to_data\n",
        "    \n",
        "i=1\n",
        "batchsize=1000\n",
        "# total test images = 24045\n",
        "for i in range(1,25000,batchsize):\n",
        "    print(i)\n",
        "    id_to_data= Imagegen(i,batchsize)\n",
        "    f=open(\"id_to_data\",\"wb+\")\n",
        "    pickle.dump(id_to_data,f,protocol=4)\n",
        "    with open(\"gdrive/My Drive/final_id_to_data_test_rem\", \"ab\") as myfile, open(\"id_to_data\", \"rb\") as file2:\n",
        "      myfile.write(file2.read())\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuZvIjUMll06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TEST"
      ]
    },
    {
      "metadata": {
        "id": "KIFyYsTktVPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "import random\n",
        "import math\n",
        "import keras.losses\n",
        "import tensorflow as tf\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "\n",
        "def smooth_l1_loss(true_box,pred_box):\n",
        "    loss=0.0\n",
        "    for i in range(4):\n",
        "        residual=tf.abs(true_box[:,i]-pred_box[:,i]*224)\n",
        "        condition=tf.less(residual,1.0)\n",
        "        small_res=0.5*tf.square(residual)\n",
        "        large_res=residual-0.5\n",
        "        loss=loss+tf.where(condition,small_res,large_res)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def Normalize(image,mean,std):\n",
        "    for channel in range(3):\n",
        "        image[:,:,channel]=(image[:,:,channel]-mean[channel])/std[channel]\n",
        "    return image\n",
        "\n",
        "def my_metric(labels,predictions):\n",
        "    threshhold=0.75\n",
        "    x=predictions[:,0]*224\n",
        "    x=tf.maximum(tf.minimum(x,224.0),0.0)\n",
        "    y=predictions[:,1]*224\n",
        "    y=tf.maximum(tf.minimum(y,224.0),0.0)\n",
        "    width=predictions[:,2]*224                                         # we need width + x\n",
        "    width=tf.maximum(tf.minimum(width,224.0),0.0)\n",
        "    height=predictions[:,3]*224                 \n",
        "    height=tf.maximum(tf.minimum(height,224.0),0.0) # we need height +y\n",
        "    label_x=labels[:,0]\n",
        "    label_y=labels[:,1]\n",
        "    label_width=labels[:,2]\n",
        "    label_height=labels[:,3]\n",
        "    a1=tf.multiply(width,height)\n",
        "    a2=tf.multiply(label_width,label_height)\n",
        "    x1=tf.maximum(x,label_x)\n",
        "    y1=tf.maximum(y,label_y)\n",
        "    x2=tf.minimum(x+width,label_x+label_width)\n",
        "    y2=tf.minimum(y+height,label_y+label_height)\n",
        "    IoU=tf.abs(tf.multiply((x1-x2),(y1-y2)))/(a1+a2-tf.abs(tf.multiply((x1-x2),(y1-y2))))\n",
        "    condition=tf.less(threshhold,IoU)\n",
        "    sum=tf.where(condition,tf.ones(tf.shape(condition)),tf.zeros(tf.shape(condition)))\n",
        "    return tf.reduce_mean(sum)\n",
        "\n",
        "def smooth_l1_loss(true_box,pred_box):\n",
        "    loss=0.0\n",
        "    for i in range(4):\n",
        "        residual=tf.abs(true_box[:,i]-pred_box[:,i]*224)\n",
        "        condition=tf.less(residual,1.0)\n",
        "        small_res=0.5*tf.square(residual)\n",
        "        large_res=residual-0.5\n",
        "        loss=loss+tf.where(condition,small_res,large_res)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "  \n",
        "k=1\n",
        "f=open(\"gdrive/My Drive/final_id_to_data_test\",\"rb\")\n",
        "\n",
        "\n",
        "with open(\"gdrive/My Drive/testid.txt\") as g:\n",
        "        lines=g.read().splitlines()\n",
        "result_file =  open(\"gdrive/My Drive/final.csv\", \"a\")\n",
        "\n",
        "#Total test images = 24045\n",
        "\n",
        "for j in range (0,25):\n",
        "    data = pickle.load(f)\n",
        "    if j==24:\n",
        "      index = [i for i in range(45)]\n",
        "    else:\n",
        "      index = [i for i in range(1000)]\n",
        "    model=keras.models.load_model(\"gdrive/My Drive/model.h5\",custom_objects = {'smooth_l1_loss': smooth_l1_loss,'my_metric':my_metric})\n",
        "#keras.losses.smooth_l1_loss = smooth_l1_loss\n",
        "    result=model.predict(data[index,:,:,:])\n",
        "    mean=[0.485,0.456,0.406]\n",
        "    std=[0.229,0.224,0.225]\n",
        "    for i in index:\n",
        "      print(\"Predicting \"+str(k)+\"th image.\")\n",
        "      image=data[i]\n",
        "      prediction=result[i]\n",
        "      for channel in range(3):\n",
        "          image[:,:,channel]=image[:,:,channel]*std[channel]+mean[channel]\n",
        "\n",
        "      image=image*255\n",
        "      image=image.astype(np.uint8)\n",
        "      plt.imshow(image)\n",
        "\n",
        "\n",
        "      plt.gca().add_patch(plt.Rectangle((prediction[0]*224,prediction[1]*224),prediction[2]*224,prediction[3]*224,fill=False,edgecolor='green',linewidth=2,alpha=0.5))\n",
        "      plt.show()\n",
        "      plt.savefig(\"./prediction/\"+str(k)+\".png\")\n",
        "      plt.cla()\n",
        "\n",
        "\n",
        "      x1 = ((prediction[0])*640)\n",
        "      x2 = (x1+(prediction[2])*640)\n",
        "      y1 = ((prediction[1])*480)\n",
        "      y2 = (y1+ (prediction[3])*480)\n",
        "      id,path=lines[k-1].split(\" \",1)  \n",
        "      k=k+1\n",
        "      result_file.write(path.rstrip()+ \",\" + str(x1)+ \",\"+str(x2) + \",\" + str(y1) + \",\" + str(y2) + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}